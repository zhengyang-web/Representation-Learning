# 我的神经网络大作业：PCA + 自动编码器

这是我这学期神经网络课的大作业记录，主要做了两部分：PCA/Kernel PCA 降维，还有一堆自动编码器的实验。把流程、结果和踩坑都写下来，方便自己复盘，也给同学参考。


1. PCA 和 Kernel PCA（10 分）

用的是 MNIST 手写数字，每张图 28×28。先做标准 PCA，看前两个主成分，画散点图和重构效果。Kernel PCA 用 RBF 核，调了几档 gamma，发现太大会过拟合成噪点。最后得出的结论是：只取两个主成分分类效果很一般（准确率 30% 左右），不过数字分布还是挺直观的。

2. 自动编码器（10 分）

结构就是全连接：`784 → 512 → 256 → 128 → 32`，编码再解码回来。损失函数试了 MSE、BCE、Smooth L1，BCE 效果最好。正则化方面，L1 会让权值更稀疏，但太大会崩；L2 稳定一些；稀疏正则需要小心系数。训练时用了 Adam + StepLR，学习率别设太大。

3. 选做部分（加分项）

降噪自动编码器是在输入端加高斯噪声，重构出来比原图干净。变分自动编码器（VAE）虽然实现了，但 KL loss 特别难调，最后验证损失超级大，后续还得改。卷积自动编码器把全连接换成卷积加反卷积，效果比基础版好一点。还试了 5 层隐藏层的深层网络，训练更慢，也更容易过拟合。最后做了个小排名和表格，总结哪个配置更靠谱。

## 项目结构

```
├── neural_network_assignment.py  # 主脚本，所有实验都在里面
├── requirements.txt              # 需要的库
└── README.md                     # 就是这个文档
```

## 运行方式

我是在 Python 3.10 上跑的，PyTorch 直接用官方轮子。

```bash
pip install -r requirements.txt
python neural_network_assignment.py
```

第一次跑会自动下载 MNIST，大概 5~10 分钟能跑完一次完整流程。

## 实验结果速记

验证损失最佳的是降噪自动编码器（0.462），普通版 0.467，深层版 0.615，VAE 直接爆炸。PCA 的前两维解释方差是 11.01%，拿来分类基本不行，但可视化还挺好看。自动编码器输出的图都放在 `outputs/` 目录（脚本会自动建文件夹）。

## 一些小心得

如果只想调某个实验，可以去脚本顶部把不需要的部分注释掉，不然一次跑太久。VAE 的 loss 写法一定要注意，别把 KL loss 算错，我一开始少了一项，调了半天。Kernel PCA 的 gamma 建议先用 1e-3、1e-4 试，太小没效果，太大直接爆。GPU 能用就用，CPU 训练真慢，我在宿舍台式机上用 3060 算的，还算快。

## 可能遇到的坑

pip 装 torch 太慢的话，可以提前下载离线包，或者直接换成清华源。如果训练跑到一半崩了，多半是内存吃满，调小 batch size 或者只抽一部分数据就好。VAE 如果 loss 不收敛，先检查是不是重参数化那里写错了。


## 最后

大家加油，期末顺利！
